# -*- coding: utf-8 -*-
"""Module3_Yasamin_Abbaszadegan_Basic_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13elWz0HeYkLBp0o2wRKyrysOxk_ttahT

# Getting the data
"""

!git clone https://github.com/YasaminAbbaszadegan/NLP_Forum_Recomendation_Engine.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/NLP_Forum_Recomendation_Engine/Dataset

ls

import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

df=pd.read_csv('tudiabetes_20.csv',index_col=0) #MyDF_final_main_comment.csv

#df=df.drop(['tags'],axis=1)
df

"""# Remove '[ ]' in Tags and Post Replies"""

df['Tags'] = df['Tags'].str.replace("[", "")
df['Tags'] = df['Tags'].str.replace("]", "")
df['Tags'] = df['Tags'].str.replace("'", "")
df['Post Replies'] = df['Post Replies'].str.replace("[", "")
df['Post Replies'] = df['Post Replies'].str.replace("]", "")
df['Post Replies'] = df['Post Replies'].str.replace("'", "")

df.head()

df['Category'].unique()

df['Category'].nunique()

"""# Data Visualization

Its a little imbalanced
"""

import matplotlib.pyplot as plt
plt.figure(figsize=(10,4))
df['Category'].value_counts().plot(kind='bar');

"""# Remove df['category']='Social Clubs','Welcome','Tudiabetes website'"""

df_cat = df[df.Category != 'Social Clubs']
df_cat = df_cat[df.Category != 'TuDiabetes Website']
df_cat = df_cat[df.Category != 'Welcome']

df_cat

print("After removing 'Social Clubs', 'TuDiabetes Website', 'Welcome' categories we remove vvv rows ")
len(df)-len(df_cat)

my_categories = [ 'Type 1 and LADA',
'Type 2', 'Diabetes and Pregnancy', 'Community', 'Food',
'Treatment', 'Diabetes Technology']

"""# Create combined dataset 

df_cat['comment_topic'],

df_cat['comment_tag'],

df_cat['comment_lead']
"""

df_cat['comment_topic'] = df_cat['Topic Title'] + ' ' + df_cat['Tags'] + ' ' + df_cat['Leading Post'] + ' ' + df_cat['Post Replies']
df_cat['comment_tag'] = df_cat['Tags'] + ' ' + df_cat['Leading Post'] + ' ' + df_cat['Post Replies']
df_cat['comment_lead'] = df_cat['Leading Post'] + ' ' + df_cat['Post Replies']
df_cat.head()

"""# WordCloud_ df_cat['comment_lead'] """

fig, axs = plt.subplots(4,2, figsize=(50, 50), facecolor='w', edgecolor='k')
# fig.subplots_adjust(hspace = .5, wspace=.001)


for axs, x in zip(axs.ravel(),df_cat['Category'].unique()):
  wc = WordCloud(background_color="white", max_words=2000, stopwords=stop_words,
                   max_font_size=40, random_state=42)
  wc.generate(df_cat.comment_lead[(df_cat.comment_lead.notnull()) & (df_cat['Category'] == x)].to_string())
   
  axs.imshow(wc)
  axs.set_title(str(x))

"""# WordCloud_ df_cat['comment_tag']"""

fig, axs = plt.subplots(4,2, figsize=(50, 50), facecolor='w', edgecolor='k')
# fig.subplots_adjust(hspace = .5, wspace=.001)


for axs, x in zip(axs.ravel(),df_cat['Category'].unique()):
  wc = WordCloud(background_color="white", max_words=2000, stopwords=stop_words,
                   max_font_size=40, random_state=42)
  wc.generate(df_cat.comment_tag[(df_cat.comment_tag.notnull()) & (df_cat['Category'] == x)].to_string())
   
  axs.imshow(wc)
  axs.set_title(str(x))

"""# Clean data"""

!pip install contractions
import contractions

import string

def clean_text(text):
    #Make text lowercase, remove punctuation and remove words containing numbers.
    text=re.sub("[\(\[].*?[\)\]]", "",text)
    text=re.sub(r'#\w+ ?', '', text ) ## remove hashtag words
    text = re.sub('<.*?>','', text)
    text = text.lower()
    text = re.sub('[%s]' % re.escape(string.punctuation), '',text)#remove punctuation
   # Get rid of some additional punctuation.
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub(' — ', '', text)
    return (text)

clean = lambda x: clean_text(x)

def full_clean(s):
  df = s.apply(lambda x: [contractions.fix(word) for word in str(x).split()])
  df= df.map(lambda x: ' '.join(x))
  stop_words.update(['hi','hello']) 

  df=df.apply(clean)
  df = df.apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))
  
  return(pd.DataFrame(df))

def clean_text1(text):
    #Make text lowercase, remove punctuation and remove words containing numbers.
    # text = re.compile('<.*?>')
    text=re.sub("[\(\[].*?[\)\]]", "",text)
    text=re.sub(r'#\w+ ?', '', text ) ## remove hashtag words
    text = re.sub('<.*?>','', text)
    text = text.lower()
    text = re.sub('[%s]' % re.escape(string.punctuation), '',text)#remove punctuation
    # text = re.sub('\w*\d\w*', '', text)
   # Get rid of some additional punctuation.
    text = re.sub('[‘’“”…]', '', text)
    text = re.sub(' — ', '', text)
    return (text)

clean1 = lambda x: clean_text1(x)

def full_clean1(s):
  df = s.apply(lambda x: [contractions.fix(word) for word in str(x).split()])
  df= df.map(lambda x: ' '.join(x))
  df=df.apply(clean1)
  return(pd.DataFrame(df))

df_cat['comment_topic_C'] =full_clean(df_cat['comment_topic'])
df_cat['comment_tag_C'] = full_clean(df_cat['comment_tag'])
df_cat['comment_lead_C']= full_clean(df_cat['comment_lead'])

df_cat['comment_topic_C1'] =full_clean1(df_cat['comment_topic'])
df_cat['comment_tag_C1'] = full_clean1(df_cat['comment_tag'])
df_cat['comment_lead_C1']= full_clean1(df_cat['comment_lead'])

df_cat.head()

"""# Prepare for Modeling"""

X = df_cat['comment_lead_C']
y = df_cat['Category']
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.25, random_state = 42)

X0 = df_cat['comment_topic_C1']
y0 = df_cat['Category']
X_train0, X_test0, y_train0, y_test0 = train_test_split(X0, y0,stratify=y0, test_size=0.25, random_state = 42)

X1 = df_cat['comment_tag_C']
y1 = df_cat['Category']
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1,stratify=y1, test_size=0.25, random_state = 42)

X2 = df_cat['comment_topic_C']
y2 = df_cat['Category']
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2,stratify=y2, test_size=0.25, random_state = 42)

"""# SVM

### X1
"""

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

svc = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
                ('svc',SVC())])
svc.fit(X_train1, y_train1)


from sklearn.metrics import classification_report
y_pred = svc.predict(X_test1)

print('accuracy %s' % accuracy_score(y_test1,y_pred))
res_svc_test1 = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred = svc.predict(X_train1)
res_svc_train1 = accuracy_score(y_train1,y_pred)
print('accuracy %s' % res_svc_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

# print(svc.score(X_test1,y_test1))
# print(svc.score(X_train1,y_train1))

"""### X2"""

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

svc = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
                ('svc',SVC())])
svc.fit(X_train2, y_train2)


from sklearn.metrics import classification_report
y_pred2 = svc.predict(X_test2)

print('test accuracy %s' % accuracy_score(y_test2,y_pred2))
res_svc_test2 = accuracy_score(y_test2,y_pred2)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred2= svc.predict(X_train2)
res_svc_train2 = accuracy_score(y_train2,y_pred)
print('trained accuracy %s' % res_svc_train2)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

"""### X0"""

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

svc = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
                ('svc',SVC())])
svc.fit(X_train0, y_train0)


from sklearn.metrics import classification_report
y_pred0 = svc.predict(X_test0)

print('test accuracy %s' % accuracy_score(y_test0,y_pred0))
res_svc_test0 = accuracy_score(y_test0,y_pred0)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred0= svc.predict(X_train0)
res_svc_train0 = accuracy_score(y_train0,y_pred0)
print('trained accuracy %s' % res_svc_train0)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

"""# Naive Bayes Classifier for Multinomial Models

### CountVectorizer + TF-IDFTransformer + MultinomialNB

The TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation. So although both the CountVectorizer and TfidfTransformer (with use_idf=False) produce term frequencies, TfidfTransformer is normalizing the count. 
https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value

### X1
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

nb = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
               ('clf', MultinomialNB()),
              ])
nb.fit(X_train1, y_train1)


from sklearn.metrics import classification_report
y_pred = nb.predict(X_test1)

print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_nb_test1 = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred = nb.predict(X_train1)
res_nb_train1 = accuracy_score(y_train1,y_pred)
print('Trained accuracy %s' % res_nb_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

"""### X2 """

nb.fit(X_train2, y_train2)


from sklearn.metrics import classification_report
y_pred = nb.predict(X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_nb_test2= accuracy_score(y_test2,y_pred)
# print(classification_report(y_test2, y_pred,target_names=my_categories))
y_pred = nb.predict(X_train2)
res_nb_train2 = accuracy_score(y_train2,y_pred)
print('Trained accuracy %s' % res_nb_train2)
# print(classification_report(y_train2, y_pred,target_names=my_categories))

"""### X0"""

nb.fit(X_train0, y_train0)


from sklearn.metrics import classification_report
y_pred = nb.predict(X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred))
res_nb_test0 = accuracy_score(y_test0,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))

y_pred = nb.predict(X_train0)
res_nb_train0 = accuracy_score(y_train0,y_pred)
print('Trained accuracy %s' % res_nb_train0)
# print(classification_report(y_train0, y_pred,target_names=my_categories))

"""#### grid **search**"""

from sklearn.model_selection import GridSearchCV
nb = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
               ('clf', MultinomialNB()),
              ])

parameters = {'vect__ngram_range': [(1, 1), (1, 2),(2,2),(2,3)],
              'tfidf__use_idf': (True, False),
               'clf__alpha': (1e-2, 1e-3),
 }

gs_clf = GridSearchCV(nb, parameters,scoring='accuracy', n_jobs=-1)

gs_clf.fit(X_train1, y_train1)


from sklearn.metrics import classification_report
y_pred = gs_clf.predict(X_test1)

print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_nb_test = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred = gs_clf.predict(X_train1)
res_nb_train = accuracy_score(y_train1,y_pred)
print('Trained accuracy %s' % res_nb_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

gs_clf.best_score_

gs_clf.best_params_

"""# SGDClassifier

### CountVectorizer + TF-IDFTransformer + SGDClassifier

### X1
"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='modified_huber', penalty='l1',alpha=1e-3, random_state=42, max_iter=10, tol=None)),
               ])
sgd.fit(X_train1, y_train1)

y_pred = sgd.predict(X_test1)

print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_SGD_test1 = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))

y_pred = sgd.predict(X_train1)
res_SGD_train1 = accuracy_score(y_train1,y_pred)
print('Trained accuracy %s' % res_SGD_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

"""### X0"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='modified_huber', penalty='l1',alpha=1e-3, random_state=42, max_iter=10, tol=None)),
               ])
sgd.fit(X_train0, y_train0)

y_pred = sgd.predict(X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred))
res_SGD_test0 = accuracy_score(y_test0,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))
y_pred = sgd.predict(X_train0)
res_SGD_train0 = accuracy_score(y_train0,y_pred)
print('Trained accuracy %s' % res_SGD_train0)
# print(classification_report(y_train0, y_pred,target_names=my_categories))

"""he possible options are 'hinge', 'log', 'modified_huber',
'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',
'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.

The 'log' loss gives logistic regression, a probabilistic classifier.
'modified_huber' is another smooth loss that brings tolerance to
outliers as well as probability estimates.
'squared_hinge' is like hinge but is quadratically penalized.
'perceptron' is the linear loss used by the perceptron algorithm.
The other losses are designed for regression but can be useful in
classification as well; see SGDRegressor for a description.


penalty : {'l2', 'l1', 'elasticnet'}, default='l2'
    The penalty (aka regularization term) to be used. Defaults to 'l2'
    which is the standard regularizer for linear SVM models. 'l1' and
    'elasticnet' might bring sparsity to the model (feature selection)
    not achievable with 'l2'.

### X2
"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='modified_huber', penalty='l1',alpha=1e-3, random_state=42, max_iter=10, tol=None)),
               ])
sgd.fit(X_train2, y_train2)

y_pred = sgd.predict(X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_SGD_test2 = accuracy_score(y_test2,y_pred)
# print(classification_report(y_test2, y_pred,target_names=my_categories))
y_pred = sgd.predict(X_train2)
res_SGD_train2 = accuracy_score(y_train2,y_pred)
print('Trained accuracy %s' % res_SGD_train2)
# print(classification_report(y_train2, y_pred,target_names=my_categories))

"""### Gridsearch

https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html
"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier()),
               ])


parameters = {'vect__max_df': (0.5, 0.75, 1.0),
    # 'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    # 'tfidf__use_idf': (True, False),
    # 'tfidf__norm': ('l1', 'l2'),
    'clf__max_iter': (20,),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    # 'clf__max_iter': (10, 50, 80),
 }

gs_clf = GridSearchCV(sgd, parameters,scoring='accuracy', n_jobs=-1)

gs_clf.fit(X_train1, y_train1)



y_pred = gs_clf.predict(X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_SGD_testG = accuracy_score(y_test2,y_pred)
# print(classification_report(y_test2, y_pred,target_names=my_categories))
y_pred = gs_clf.predict(X_train2)
res_SGD_trainG = accuracy_score(y_train2,y_pred)
print('Trained accuracy %s' % res_SGD_train2)
# print(classification_report(y_train2, y_pred,target_names=my_categories))

gs_clf.best_score_

gs_clf.best_params_

"""# Logistic Regression

### CountVectorizer + TF-IDFTransformer + Logistic Regression

https://stackoverflow.com/questions/20894671/speeding-up-sklearn-logistic-regression

solver---> https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

### X1
"""

from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import TruncatedSVD
logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                # ('svd', TruncatedSVD(n_components = 100)),
                ('clf', LogisticRegression(penalty='l1',solver='liblinear')),#  C--> Inverse of regularization strength; must be a positive float.
                                                              # LogisticRegression now says that njobs chooses the "Number of CPU cores used during the cross-validation loop"
                                                              # whereas the other two items cited in the original response, RandomForestClassifier() 
                                                              #and RandomForestRegressor(), both state that njobs specifies "The number of jobs to run in parallel for both fit and predict". In other words, the deliberate contrast in phrasing here seems to be pointing out that the njobs parameter in LogisticRegression(), while now implemented, is not really implemented as completely, or in the same way, as in the other two cases.
                                                           ]) #penalty='l1',solver='liblinear',n_jobs=1, C=100

logreg.fit(X_train1, y_train1)

y_pred = logreg.predict (X_test1)


print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_logreg_test1 = accuracy_score(y_test1,y_pred)

# print(classification_report(y_test1, y_pred,target_names=my_categories))

y_pred = sgd.predict(X_train1)

res_logreg_train1 = accuracy_score(y_train1,y_pred)
print('Trained accuracy %s' % res_logreg_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

# import seaborn as sns
# conf_mat = confusion_matrix(y_test1, y_pred)
# fig, ax = plt.subplots(figsize=(10,10))
# sns.heatmap(conf_mat, annot=True, fmt='d',cmap="YlGnBu",
#             xticklabels=my_categories, yticklabels=my_categories) ## color--> https://python-graph-gallery.com/92-control-color-in-seaborn-heatmaps/
# plt.ylabel('Actual')
# plt.xlabel('Predicted')
# plt.show()

"""### X2"""

from sklearn.linear_model import SGDClassifier

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(penalty='l1',solver='liblinear')),
               ])
logreg.fit(X_train2, y_train2)

y_pred = logreg.predict(X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_logreg_test2 = accuracy_score(y_test2,y_pred)

# print(classification_report(y_test2, y_pred,target_names=my_categories))
y_pred = sgd.predict(X_train2)

res_logreg_train2 = accuracy_score(y_train2,y_pred)
print('Trained accuracy %s' % res_logreg_train2)
# print(classification_report(y_train2, y_pred,target_names=my_categories))

"""### X0"""

from sklearn.linear_model import SGDClassifier

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(penalty='l1',solver='liblinear')),
               ])
logreg.fit(X_train0, y_train0)

y_pred = logreg.predict(X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred))
res_logreg_test0 = accuracy_score(y_test0,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))


y_pred = sgd.predict(X_train2)

res_logreg_train0 = accuracy_score(y_train0,y_pred)
print('Trained accuracy %s' % res_logreg_train0)
# print(classification_report(y_train0, y_pred,target_names=my_categories))

"""# XGBClassifier

#### CountVectorizer + TF-IDFTransformer + XGBClassifier

**for overfitting**
https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model

**Parameters of XGB** 
https://xgboost.readthedocs.io/en/latest/parameter.html

###X1
"""

from xgboost import XGBClassifier

XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
XGB.fit(X_train1, y_train1)

y_pred = XGB.predict (X_test1)

print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_XGB_test1 = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred1 = XGB.predict(X_train1)

res_XGB_train1 = accuracy_score(y_train1,y_pred1)
print('Trained accuracy %s' % res_XGB_train1)
# print(classification_report(y_train1, y_pred1,target_names=my_categories))

"""### X2"""

from xgboost import XGBClassifier

XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
XGB.fit(X_train2, y_train2)

y_pred = XGB.predict (X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_XGB_test2 = accuracy_score(y_test2,y_pred)
# print(classification_report(y_test2, y_pred,target_names=my_categories))
y_pred2 = XGB.predict(X_train2)

res_XGB_train2 = accuracy_score(y_train2,y_pred2)
print('Trained accuracy %s' % res_XGB_train2)
# print(classification_report(y_train2, y_pred2,target_names=my_categories))

"""### X0"""

from xgboost import XGBClassifier

XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
XGB.fit(X_train0, y_train0)

y_pred = XGB.predict (X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred))
res_XGB_test0 = accuracy_score(y_test0,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))

y_pred0 = XGB.predict(X_train0)

res_XGB_train0 = accuracy_score(y_train0,y_pred0)
print('Trained accuracy %s' % res_XGB_train0)
# print(classification_report(y_train0, y_pred0,target_names=my_categories))

"""### GridSearchCV"""

print(XGB.get_params().keys())

import time
start = time.process_time()


from sklearn.model_selection import RandomizedSearchCV
XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier()),
               ])


parameters = { #'vect__max_df': (0.5, 0.75, 1.0),
    
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    'tfidf__use_idf': (True, False),
    # 'tfidf__norm': ('l1', 'l2'),
    'clf__max_depth':(1,2,3),
    # 'clf__min_child_weight': (1,2),
    
    # 'clf_subsample': 1,
    # 'clf_colsample_bytree': 1,
    'clf__gamma':(.1,.5,.8),
    # Other parameters
    'clf__objective':('reg:linear','multi:softprob'),
 }


gs_clf = GridSearchCV(XGB, parameters,scoring='accuracy')

gs_clf.fit(X_train2, y_train2)


y_pred = gs_clf.predict (X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred))
res_XGB_test2_G = accuracy_score(y_test2,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))

y_pred2 = XGB.predict(X_train2)

res_XGB_train2_G = accuracy_score(y_train2,y_pred2)
print('Trained accuracy %s' % res_XGB_train2_G)
# print(classification_report(y_train0, y_pred0,target_names=my_categories))
print(time.process_time() - start)

from xgboost import XGBClassifier

XGB = Pipeline([('vect', CountVectorizer(analyzer='word', token_pattern=r'\w{1,}', ngram_range=(2,4), max_features=5000)),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
XGB.fit(X_train0, y_train0)

y_pred = XGB.predict (X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred))
res_XGB_test = accuracy_score(y_test0,y_pred)
# print(classification_report(y_test0, y_pred,target_names=my_categories))

y_pred0 = XGB.predict(X_train0)

res_XGB_train = accuracy_score(y_train0,y_pred0)
print('Trained accuracy %s' % res_XGB_train)
# print(classification_report(y_train0, y_pred0,target_names=my_categories))

"""# RandomForestClassifier

#### CountVectorizer + TF-IDFTransformer + RandomForestClassifier

### X1
"""

from sklearn.ensemble import RandomForestClassifier

RFC = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', RandomForestClassifier(n_estimators=1500,max_depth=1,min_samples_leaf=30)),
               ])
RFC.fit(X_train1, y_train1)

y_pred = RFC.predict (X_test1)

print('Test accuracy %s' % accuracy_score(y_test1,y_pred))
res_RFC_test1 = accuracy_score(y_test1,y_pred)
# print(classification_report(y_test1, y_pred,target_names=my_categories))
y_pred = RFC.predict(X_train1)
res_RFC_train1 = accuracy_score(y_train1, y_pred)
print('Trained accuracy %s' % res_RFC_train1)
# print(classification_report(y_train1, y_pred,target_names=my_categories))

"""n_estimators: The more trees, the less likely the algorithm is to overfit. So try increasing this parameter. The lower this number, the closer the model is to a decision tree, with a restricted feature set.
max_features: You should try reducing this number. This defines how many features each tree is randomly assigned. 
max_depth: This parameter will reduce the complexity of the learned models, lowering over fitting risk. 
min_samples_leaf: Try setting these values greater than one. This has a similar effect to the max_depth parameter, it means the branch will stop splitting once the leaves have that number of samples each.

### X2
"""

from sklearn.ensemble import RandomForestClassifier

RFC = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', RandomForestClassifier(n_estimators=1500,max_depth=1,min_samples_leaf=30)),
               ])
RFC.fit(X_train2, y_train2)

y_pred2 = RFC.predict (X_test2)

print('Test accuracy %s' % accuracy_score(y_test2,y_pred2))
res_RFC_test2 = accuracy_score(y_test2,y_pred2)
# print(classification_report(y_test2, y_pred2,target_names=my_categories))

y_pred2 = RFC.predict(X_train2)
res_RFC_train2 = accuracy_score(y_train2, y_pred2)
print('Trained accuracy %s' % res_RFC_train2)
# print(classification_report(y_train2, y_pred2,target_names=my_categories))

"""### X0"""

from sklearn.ensemble import RandomForestClassifier

RFC = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', RandomForestClassifier(n_estimators=1500,max_depth=1,min_samples_leaf=30)),
               ])
RFC.fit(X_train0, y_train0)

y_pred0 = RFC.predict (X_test0)

print('Test accuracy %s' % accuracy_score(y_test0,y_pred0))
res_RFC_test0 = accuracy_score(y_test0,y_pred0)
# print(classification_report(y_test2, y_pred2,target_names=my_categories))

y_pred0 = RFC.predict(X_train0)
res_RFC_train0 = accuracy_score(y_train0, y_pred0)
print('Trained accuracy %s' % res_RFC_train0)
# print(classification_report(y_train2, y_pred2,target_names=my_categories))



"""# Comparing multiple models accuracy score"""

import pandas as pd

results = pd.DataFrame({'Model': ['Naive Bayes MultinomialNB', 'Linear SVM', 'SGDClassifier','Logistic Regression','XGBClassifier','RandomForestClassifier'],
                         'Test_X1': [res_nb_test1,res_svc_test1,res_SGD_test1,res_logreg_test1,res_XGB_test1,res_RFC_test1],
                         'Train_X1':[res_nb_train1,res_svc_train1,res_SGD_train1,res_logreg_train1,res_XGB_train1,res_RFC_train1],
                         'Test_X2': [res_nb_test2,res_svc_test2,res_SGD_test2,res_logreg_test2,res_XGB_test2,res_RFC_test2],
                         'Train_X2':[res_nb_train2,res_svc_train2,res_SGD_train2,res_logreg_train2,res_XGB_train2,res_RFC_train2],
                         'Test_X0': [res_nb_test0,res_svc_test0,res_SGD_test0,res_logreg_test0,res_XGB_test0,res_RFC_test0],
                         'Train_X0':[res_nb_train0,res_svc_train0,res_SGD_train0,res_logreg_train0,res_XGB_train0,res_RFC_train0],
                        })
results.set_index('Model')
results.sort_values(by='Test_X2')

"""x0=comment_topic_with_stop_words,

x1=comment_topic_without_stop_words,

x2=comment_tag_C_without_stop_words
"""

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (20,10)
results.sort_values(by='Test_X2',ascending=False).plot.bar(x='Model')

"""# Cross Validation_X2
https://scikit-learn.org/stable/modules/cross_validation.html,

https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85

### CV_NB
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import cross_val_score

nb = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
               ('clf', MultinomialNB()),
              ])


cv_res_nb = cross_val_score(nb, X_train2, y_train2,scoring='accuracy', cv=10)
mean_cv_res_nb = np.mean(cv_res_nb)
print(mean_cv_res_nb)

std_cv_res_nb = np.std(cv_res_nb)
print(std_cv_res_nb)

cv_res_NB = cross_val_score(nb, X2, y2,scoring='accuracy', cv=10)
std_cv_res_NB=np.std(cv_res_NB)
mean_cv_res_NB = np.mean(cv_res_NB)
print(mean_cv_res_NB)
print(std_cv_res_NB)

"""### CV_SVM"""

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

svc = Pipeline([('vect', CountVectorizer()), ##
               ('tfidf', TfidfTransformer()), ##
                ('svc',SVC())])



cv_res_svc = cross_val_score(svc, X_train2, y_train2,scoring='accuracy', cv=10)
mean_cv_res_svc = np.mean(cv_res_svc)
print(mean_cv_res_svc)

std_cv_res_svc = np.std(cv_res_svc)
print(std_cv_res_svc)

cv_res_SVC = cross_val_score(svc, X2, y2,scoring='accuracy', cv=10)
std_cv_res_SVC=np.std(cv_res_SVC)
mean_cv_res_SVC = np.mean(cv_res_SVC)
print(mean_cv_res_SVC)
print(std_cv_res_SVC)

"""### CV_RF"""

from sklearn.ensemble import RandomForestClassifier

RFC = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', RandomForestClassifier(n_estimators=1500,max_depth=1,min_samples_leaf=30)),
               ])

cv_res_RFC = cross_val_score(RFC, X_train2, y_train2,scoring='accuracy', cv=10)
mean_cv_res_RFC = np.mean(cv_res_RFC)
print(mean_cv_res_RFC)

std_cv_res_RFC = np.std(cv_res_RFC)
print(std_cv_res_RFC)

cv_res_rfc = cross_val_score(RFC, X2, y2,scoring='accuracy', cv=10)
std_cv_res_rfc=np.std(cv_res_rfc)
mean_cv_res_rfc = np.mean(cv_res_rfc)
print(mean_cv_res_rfc)
print(std_cv_res_rfc)

"""### CV_SGDClassifier"""

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_score
sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='modified_huber', penalty='l1',alpha=1e-3, max_iter=10, tol=None)), # random_state=42
               ])

cv_res_sgd = cross_val_score(sgd, X_train2, y_train2,scoring='accuracy', cv=10)
mean_cv_res_sgd = np.mean(cv_res_sgd)
print(mean_cv_res_sgd)

std_cv_res_sgd=np.std(cv_res_sgd)
print(std_cv_res_sgd)

cv_res_SGD = cross_val_score(sgd, X2, y2,scoring='accuracy', cv=10)
std_cv_res_SGD=np.std(cv_res_SGD)
mean_cv_res_SGD = np.mean(cv_res_SGD)
print(mean_cv_res_SGD)
print(std_cv_res_SGD)

"""### CV_LogisticRegression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(penalty='l1',solver='liblinear')),
               ])
cv_res_logreg = cross_val_score(logreg, X_train2, y_train2,scoring='accuracy', cv=10)
std_cv_res_logreg=np.std(cv_res_logreg)
mean_cv_res_logreg = np.mean(cv_res_logreg)
print(mean_cv_res_logreg)
print(std_cv_res_logreg)

cv_res_logreG = cross_val_score(logreg, X2, y2,scoring='accuracy', cv=10)
std_cv_res_logreG=np.std(cv_res_logreG)
mean_cv_res_logreG = np.mean(cv_res_logreG)
print(mean_cv_res_logreG)
print(std_cv_res_logreG)

"""### CV_XGBClassifier"""

from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
cv_res_XGB = cross_val_score(XGB, X_train2, y_train2,scoring='accuracy', cv=10)
mean_cv_res_XGB = np.mean(cv_res_XGB)
std_cv_res_XGB=np.std(cv_res_XGB)
print(mean_cv_res_XGB)
print(std_cv_res_XGB)

X2 = df_cat['comment_topic_C']
y2 = df_cat['Category']
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

XGB = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', XGBClassifier(objective='multi:softprob',max_depth=1,gamma=0.5,eta=0.1)),
               ])
cv_res_XGb = cross_val_score(XGB, X2, y2,scoring='accuracy', cv=10)
mean_cv_res_XGb = np.mean(cv_res_XGb)
std_cv_res_XGb=np.std(cv_res_XGb)
print(mean_cv_res_XGb)
print(std_cv_res_XGb)

"""### CV Results"""

import pandas as pd

results = pd.DataFrame({'Model': ['CV_NB', 'CV_SVM', 'CV_RF','CV_SGDClassifier','CV_LogisticRegression','CV_XGBClassifier'],
                         'Mean_Accuracy_Train_X2': [mean_cv_res_nb,mean_cv_res_svc,mean_cv_res_RFC,mean_cv_res_sgd,mean_cv_res_logreg,mean_cv_res_XGB],
                        'STD_Train_X2':[std_cv_res_nb,std_cv_res_svc,std_cv_res_RFC,std_cv_res_sgd,std_cv_res_logreg,std_cv_res_XGB],
                        'Mean_Accuracy_X2':[mean_cv_res_NB,mean_cv_res_SVC,mean_cv_res_rfc,mean_cv_res_SGD,mean_cv_res_logreg,mean_cv_res_XGb],
                        'STD_Accuracy_X2':[std_cv_res_NB,std_cv_res_SVC,std_cv_res_rfc,std_cv_res_SGD,std_cv_res_logreg,std_cv_res_XGb]
                         
                        })
results.set_index('Model')
results.sort_values(by='Mean_Accuracy_Train_X2',ascending=False)

"""# Display CV BoxPlot"""

dictionary={'CV_NB':cv_res_nb,
            'CV_SVM':cv_res_svc,
            'CV_RF':cv_res_RFC,
            'CV_SGDClassifier':cv_res_sgd,
            'CV_LogisticRegression':cv_res_logreg,
            'CV_XGBClassifier':cv_res_XGB}

df = pd.DataFrame.from_dict(dictionary)
df

import seaborn as sns
plt.rcParams["figure.figsize"] = (20,10)

sns.boxplot(x="variable", y="value", data=pd.melt(df),order=list(results.sort_values(by='Mean_Accuracy_Train_X2',ascending=False)['Model']))

sns.swarmplot(x="variable", y="value", data=pd.melt(df),order=list(results.sort_values(by='Mean_Accuracy_Train_X2',ascending=False)['Model']), color=".05",size=8)

plt.show()

